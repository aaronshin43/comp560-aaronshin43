# Morse Code Experiment

This project utilizes nanoGPT code to experiment with learning the relationship between English words and Morse code.

## Data Format

The data is generated by the `data/basic/prepare.py` script. This script concatenates predefined "building blocks" (e.g., `HELP: .... . .-.. .--.`) in a random order to create a large text file.

The format of the data is as follows:
```text
TEXT: MORSE CODE
TEXT: MORSE CODE
...
```

Running `prepare.py` generates `train.bin` and `val.bin` files, which are the text data converted into character-level integer IDs.

## Setup

### Environment Setup

First, set up a virtual environment and install the necessary packages.

```bash
python -m venv venv
# Windows
.\venv\Scripts\activate
# Mac/Linux
# source venv/bin/activate
```

Install the required packages:

```bash
pip install torch numpy tqdm wandb
```

### Data Generation

Before starting training, you need to generate the data. Run the following command inside the `morse-code` directory:

```bash
python data/basic/prepare.py
```

This command creates the necessary `.bin` files and `meta.pkl` file in the `data/basic/` folder.

## Training

Once the data is prepared, you can start training. The command below should be run from the `morse-code` directory, assuming the `comp560-nanoGPT` code exists in the parent directory (`../../comp560-nanoGPT`).

```bash
NANOGPT_CONFIG=../../comp560-nanoGPT/configurator.py python -u ../../comp560-nanoGPT/train.py config/basic.py
```

## Sampling

To generate text (sampling) using the trained model, use the following command:

```bash
NANOGPT_CONFIG=../../comp560-nanoGPT/configurator.py python -u ../../comp560-nanoGPT/sample.py config/basic.py --num_samples=1 --max_new_tokens=100 --seed=2
```

## Experiment Log

### First Experiment
*   **Dataset:** Small initial list (HELP, SOS, AI, CAT, HELLO, WORLD)
*   **Settings:** default config (max_iters = 200)
*   **Sample:**
    ```text
    HELO: .... .-. .-. .-.
    CAT: -.- -
    HELLP: .... . .-. .-.. .-.
    AI: ...
    AT: .- --.
    AI: .
    CAI: .- .- .-
    ```
*   **Observation:** The model clearly underfitted due to lack of training steps. It struggled with spelling (generated "HELO", "HELLP", "CAI") and produced incorrect Morse patterns.

### Parameter Adjustment
*   **Settings:** Increased `max_iters` to 2000.
*   **Mistake:** Left `lr_decay_iters` at 200.
*   **Sample:**
    ```text
    WORLD: .-- --- .-. .-.. -..
    HELLO: .... . .-.. .-.. ---
    HELLO: .... . .-.. .-.. ---
    SOS: ... --- ...
    AI: .- ..
    WORLD: .-- .-- --. .-.. -..
    HELP: ....
    ```
*   **Observation:** The learning rate dropped to the minimum too quickly. Accuracy improved compared to the first run, but results were inconsistent (e.g., "WORLD" has an error in this sample).

### Final Experiment
*   **Changes:**
    *   Added new words (CODE, DOG, LOVE, SKY, STAR, SUN, SEA, HI, BYE).
    *   Increased `lr_decay_iters` to 2000
.
*   **Sample:**
    ```text
    HELLO: .... . .-.. .-.. ---
    CAT: -.-. .- -
    SKY: ... -.- -.--
    STAR: ... - .- .-.
    SUN: ... ..- -.
    DOG: -.. --- --.
    SEA: ... . .-
    DOG: -.. --- --.
    BYE: -... -.-- .
    CODE: -.-. --- -.. .
    ```
*   **Observation:**
    *   Generated 5 samples (approx. 1000 tokens) with 0 errors.
    *   The model perfectly memorized the specific pairs.

## Conclusion

The model is capable of associative learning, perfectly mapping specific English keys to their Morse code values when trained sufficiently.

I learned that having `lr_decay_iters` too small compared to `max_iters` can lead to suboptimal training, as the learning rate drops to the minimum too quickly.